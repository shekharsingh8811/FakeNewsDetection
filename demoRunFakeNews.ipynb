{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import en_core_web_lg\n",
    "import en_core_web_sm\n",
    "from contractions import contractions_dict\n",
    "\n",
    "import twint\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input file with the URLs to classify\n",
    "## domain, url, urlType, title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlsFile4Demo = 'demo_input_data_v1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using twint extract the tweets that have the url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Use this script to process the datasets for FAKE, RELIABLE AND CONSPIRACY data. \n",
    "#Run the rogram with EXACTLY 5 argruments as shown below:\n",
    "#python <script> <input-file-name> <output-CSV-file> <#skipInitialRows> <#rowsToProcess> <#rowsAfterWhichToShowPrintMessageTracker>\n",
    "\n",
    "##skipInitialRows : is an integer. Is the CSV data row number (0 being for header) from which the data should be loaded. Here 0 is the header so to skip first 5 data rows: skipInitialRows = 5.\n",
    "#rowsToProcess : is an integer. If = -1 then will read in all the rows after skipping as per above parameter. Otherwise, will read in the number specified.\n",
    "#Note only if running in jupyter then uncomment the two lines for import nest_asyncio and the nest_asyncio.apply()\n",
    "\n",
    "#The packages expected by the script are:\n",
    "#import pandas as pd\n",
    "#import twint\n",
    "#import csv\n",
    "#import os\n",
    "#import sys\n",
    "#from datetime import datetime\n",
    "#import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scriptExtractTweets.py demo_input_data_v1.csv twint_data.csv 0 15 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets have been extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T features -- extract TWITTER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIpFile = 'twint_data.csv'\n",
    "ipDataUsecolsList = [ 'date' , 'user_id_str' , 'nlikes' , 'nreplies' , 'nretweets' , 'search' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipInitialRows = 0\n",
    "dfIpData = pd.read_csv(dataIpFile, usecols = ipDataUsecolsList, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIpData['domain'] = \"garbage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfIpData['dateNew'] = pd.to_datetime(dfIpData['date'])\n",
    "dfOut = dfIpData[['search', 'domain']]\n",
    "dfOut = dfOut.drop_duplicates(subset = ['search'], keep = 'first')\n",
    "dfOut = dfOut.reset_index(drop=True)\n",
    "dfOut['urlType'] = \"garbage\"\n",
    "\n",
    "####################################################\n",
    "## number of tweets  (n)\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalTweets'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalTweets'] = dfIpTemp['totalTweets']\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## totalRetweets, totalLikes, totalReplies\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalRetweets'] = dfIpTemp.groupby('search')['nretweets'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalRetweets'] =  dfIpTemp['totalRetweets']\n",
    "del(dfIpTemp)\n",
    "#\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalLikes'] = dfIpTemp.groupby('search')['nlikes'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalLikes'] =  dfIpTemp['totalLikes']\n",
    "del(dfIpTemp)\n",
    "#\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalReplies'] = dfIpTemp.groupby('search')['nreplies'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalReplies'] =  dfIpTemp['totalReplies']\n",
    "del(dfIpTemp)\n",
    "#\n",
    "\n",
    "####################################################\n",
    "## span\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfOutTemp = dfIpTemp.groupby('search')['dateNew'].agg(['min', 'max'])\n",
    "del(dfIpTemp)\n",
    "dfOutTemp.reset_index(inplace = True)\n",
    "dfOutTemp['max'] = pd.to_datetime(dfOutTemp['max'])\n",
    "dfOutTemp['min'] = pd.to_datetime(dfOutTemp['min'])\n",
    "dfOutTemp['span'] = (dfOutTemp['max'] - dfOutTemp['min']) / np.timedelta64(1, 'h') \n",
    "dfOut.insert(7, 'span', dfOut['search'].map(dfOutTemp.set_index('search')['span']))\n",
    "dfOut.loc[dfOut['span'] == 0, 'span'] = -1\n",
    "del(dfOutTemp)\n",
    "\n",
    "####################################################\n",
    "# date processing to find out weekday, weekend, day of week counts\n",
    "## process Weekday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[((dfIpTemp['tempDayOfWeek'] == 'Saturday') | (dfIpTemp['tempDayOfWeek'] == 'Sunday')), 'tempWkdayWkendType'] = 'Weekend'\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] != 'Weekend', 'tempWkdayWkendType'] = 'Weekday'\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] == 'Weekday', 'countWeekDay'] = 1\n",
    "dfIpTemp['countWeekDay'] = dfIpTemp['countWeekDay'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countWeekDay'] = dfIpTemp.groupby('search')['countWeekDay'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countWeekDay'] = dfIpTemp['countWeekDay']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Weekend counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[((dfIpTemp['tempDayOfWeek'] == 'Saturday') | (dfIpTemp['tempDayOfWeek'] == 'Sunday')), 'tempWkdayWkendType'] = 'Weekend'\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] != 'Weekend', 'tempWkdayWkendType'] = 'Weekday'\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] == 'Weekend', 'countWeekEnd'] = 1\n",
    "dfIpTemp['countWeekEnd'] = dfIpTemp['countWeekEnd'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countWeekEnd']   = dfIpTemp.groupby('search')['countWeekEnd'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countWeekEnd'] = dfIpTemp['countWeekEnd']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Monday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Monday', 'countMonday'] = 1\n",
    "dfIpTemp['countMonday'] = dfIpTemp['countMonday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countMonday'] = dfIpTemp.groupby('search')['countMonday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countMonday'] = dfIpTemp['countMonday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Tuesday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Tuesday', 'countTuesday'] = 1\n",
    "dfIpTemp['countTuesday'] = dfIpTemp['countTuesday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countTuesday'] = dfIpTemp.groupby('search')['countTuesday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countTuesday'] = dfIpTemp['countTuesday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Wednesday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Wednesday', 'countWednesday'] = 1\n",
    "dfIpTemp['countWednesday'] = dfIpTemp['countWednesday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countWednesday'] = dfIpTemp.groupby('search')['countWednesday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countWednesday'] = dfIpTemp['countWednesday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Thursday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Thursday', 'countThursday'] = 1\n",
    "dfIpTemp['countThursday'] = dfIpTemp['countThursday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countThursday'] = dfIpTemp.groupby('search')['countThursday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countThursday'] = dfIpTemp['countThursday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Friday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Friday', 'countFriday'] = 1\n",
    "dfIpTemp['countFriday'] = dfIpTemp['countFriday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countFriday'] = dfIpTemp.groupby('search')['countFriday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countFriday'] = dfIpTemp['countFriday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Saturday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Saturday', 'countSaturday'] = 1\n",
    "dfIpTemp['countSaturday'] = dfIpTemp['countSaturday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countSaturday'] = dfIpTemp.groupby('search')['countSaturday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countSaturday'] = dfIpTemp['countSaturday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Sunday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Sunday', 'countSunday'] = 1\n",
    "dfIpTemp['countSunday'] = dfIpTemp['countSunday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countSunday'] = dfIpTemp.groupby('search')['countSunday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countSunday'] = dfIpTemp['countSunday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "\n",
    "####################################################\n",
    "## average time between tweets\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfIpTemp['timeBetween2Tweets'] = dfIpTemp.groupby('search')['dateNew'].diff().fillna(0)\n",
    "dfIpTemp['timeBetween2Tweets'] = dfIpTemp['timeBetween2Tweets'] / np.timedelta64(1, 'h')\n",
    "## using sum and dividing by n-1\n",
    "dfOutTemp = dfIpData[['search']]\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOutTemp['avgTimeBetweenTweets'] = dfIpTemp.groupby('search')['timeBetween2Tweets'].transform('sum')\n",
    "dfOutTemp['countTweet'] = dfOutTemp['search'].map(dfOutTemp['search'].value_counts())\n",
    "dfOutTemp = dfOutTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfOutTemp = dfOutTemp.reset_index(drop=True)\n",
    "dfOutTemp['avgTimeBetweenTweets'] = dfOutTemp['avgTimeBetweenTweets'] / (dfOutTemp['countTweet'] - 1)\n",
    "dfOutTemp.loc[dfOutTemp['avgTimeBetweenTweets'].isnull(), 'avgTimeBetweenTweets'] = -1\n",
    "dfOut['avgTimeBetweenTweets'] = dfOutTemp['avgTimeBetweenTweets']\n",
    "del(dfOutTemp)\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## average time of next tweet\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['dateNew'] - dfIpTemp.groupby('search')['dateNew'].transform('first')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['timeFromFirstTweet']/np.timedelta64(1, 'h')\n",
    "dfIpTemp['countTweets'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "dfIpTemp['avgTimeOfNextTweet'] = dfIpTemp.groupby('search')['timeFromFirstTweet'].transform(sum)\n",
    "dfIpTemp['avgTimeOfNextTweet'] = dfIpTemp['avgTimeOfNextTweet']/(dfIpTemp['countTweets'] - 1)\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset = ['search'], keep = 'first')\n",
    "dfIpTemp.loc[dfIpTemp['avgTimeOfNextTweet'].isnull(), 'avgTimeOfNextTweet'] = -1\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut = dfOut.reset_index(drop=True)\n",
    "dfOut['avgTimeOfNextTweet'] = dfIpTemp['avgTimeOfNextTweet']\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## Time Absolute Bins\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['dateNew'] - dfIpTemp.groupby('search')['dateNew'].transform('first')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['timeFromFirstTweet']/np.timedelta64(1, 'h')\n",
    "dfIpTemp['Bins'] = \"\"\n",
    "Bins = []\n",
    "for time in dfIpTemp['timeFromFirstTweet']:\n",
    "    if time <= 6:\n",
    "        Bins.append('0 - 6')\n",
    "    elif time > 6 and time <= 12:\n",
    "        Bins.append('6 - 12')\n",
    "    elif time > 12 and time <= 18:\n",
    "        Bins.append('12 - 18')\n",
    "    elif time > 18 and time <= 24:\n",
    "        Bins.append('18 - 24')\n",
    "    else:\n",
    "        Bins.append('Greater than 24')\n",
    "dfIpTemp['Bins'] = Bins\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '0 - 6', '0 - 6 hour bin'] = 1\n",
    "dfIpTemp['0 - 6 hour bin'] = dfIpTemp['0 - 6 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '6 - 12', '6 - 12 hour bin'] = 1\n",
    "dfIpTemp['6 - 12 hour bin'] = dfIpTemp['6 - 12 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '12 - 18', '12 - 18 hour bin'] = 1\n",
    "dfIpTemp['12 - 18 hour bin'] = dfIpTemp['12 - 18 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '18 - 24', '18 - 24 hour bin'] = 1\n",
    "dfIpTemp['18 - 24 hour bin'] = dfIpTemp['18 - 24 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == 'Greater than 24', 'Greater than 24'] = 1\n",
    "dfIpTemp['Greater than 24'] = dfIpTemp['Greater than 24'].fillna(0)\n",
    "#\n",
    "dfIpTemp['count0to6'] = dfIpTemp.groupby('search')['0 - 6 hour bin'].transform('sum')\n",
    "dfIpTemp['count6to12'] = dfIpTemp.groupby('search')['6 - 12 hour bin'].transform('sum')\n",
    "dfIpTemp['count12to18'] = dfIpTemp.groupby('search')['12 - 18 hour bin'].transform('sum')\n",
    "dfIpTemp['count18to24'] = dfIpTemp.groupby('search')['18 - 24 hour bin'].transform('sum')\n",
    "dfIpTemp['count24plus'] = dfIpTemp.groupby('search')['Greater than 24'].transform('sum')\n",
    "#\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['timeAbsBin1count0to6'] = dfIpTemp['count0to6']\n",
    "dfOut['timeAbsBin2count6to12'] = dfIpTemp['count6to12']\n",
    "dfOut['timeAbsBin3count12to18'] = dfIpTemp['count12to18']\n",
    "dfOut['timeAbsBin4count18to24'] = dfIpTemp['count18to24']\n",
    "dfOut['timeAbsBin5count24plus'] = dfIpTemp['count24plus']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## avg Tweets Per Unique User\n",
    "dfIpTemp = dfIpData[['date', 'user_id_str', 'search', 'dateNew']].copy()\n",
    "dfIpTemp['user_id_str'] = dfIpTemp['user_id_str'].astype(str)\n",
    "dfIpTemp['search_user_id_str'] = dfIpTemp['search'] + '___' + dfIpTemp['user_id_str']\n",
    "dfIpTemp['countTweetsForUrl'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "dfIpTemp['countTweetsPerUser'] = dfIpTemp['search_user_id_str'].map(dfIpTemp['search_user_id_str'].value_counts())\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset = ['search_user_id_str'], keep = 'first')\n",
    "dfIpTemp['countUniqueUsersPerUrl'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "del dfIpTemp['search_user_id_str']\n",
    "dfIpTemp['avgTweetsPerUniqUser'] = dfIpTemp['countTweetsForUrl'] / dfIpTemp['countUniqueUsersPerUrl']\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset = ['search'], keep = 'first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['avgTweetsPerUniqUser'] = dfIpTemp['avgTweetsPerUniqUser']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "## normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "dfOutTempNorm = dfOut[['totalTweets', 'totalRetweets', 'totalLikes', 'totalReplies',           \\\n",
    "                    'countWeekDay', 'countWeekEnd', 'countMonday', 'countTuesday',             \\\n",
    "                    'countWednesday', 'countThursday', 'countFriday', 'countSaturday',         \\\n",
    "                    'countSunday', 'avgTweetsPerUniqUser']]\n",
    "dfOutTempNorm[['totalTweets', 'totalRetweets', 'totalLikes', 'totalReplies',                     \\\n",
    "                    'countWeekDay', 'countWeekEnd', 'countMonday', 'countTuesday',         \\\n",
    "                    'countWednesday', 'countThursday', 'countFriday', 'countSaturday',             \\\n",
    "                    'countSunday', 'avgTweetsPerUniqUser']]   \\\n",
    "= \\\n",
    "scaler.fit_transform(dfOutTempNorm[['totalTweets', 'totalRetweets', 'totalLikes', 'totalReplies', \\\n",
    "                    'countWeekDay', 'countWeekEnd', 'countMonday', 'countTuesday',  \\\n",
    "                    'countWednesday', 'countThursday', 'countFriday', 'countSaturday',      \\\n",
    "                    'countSunday', 'avgTweetsPerUniqUser']])\n",
    "#\n",
    "dfOut['normTotalTweets'] = dfOutTempNorm['totalTweets']\n",
    "dfOut['normTotalRetweets'] = dfOutTempNorm['totalRetweets']\n",
    "dfOut['normTotalLikes'] = dfOutTempNorm['totalLikes']\n",
    "dfOut['normTotalReplies'] = dfOutTempNorm['totalReplies']\n",
    "dfOut['normCountWeekDay'] = dfOutTempNorm['countWeekDay']\n",
    "dfOut['normCountWeekEnd'] = dfOutTempNorm['countWeekEnd']\n",
    "dfOut['normCountMonday'] = dfOutTempNorm['countMonday']\n",
    "dfOut['normCountTuesday'] = dfOutTempNorm['countTuesday']\n",
    "dfOut['normCountWednesday'] = dfOutTempNorm['countWednesday']\n",
    "dfOut['normCountThursday'] = dfOutTempNorm['countThursday']\n",
    "dfOut['normCountFriday'] = dfOutTempNorm['countFriday']\n",
    "dfOut['normCountSaturday'] = dfOutTempNorm['countSaturday']\n",
    "dfOut['normCountSunday'] = dfOutTempNorm['countSunday']\n",
    "dfOut['normAvgTweetsPerUniqUser'] = dfOutTempNorm['avgTweetsPerUniqUser']\n",
    "#\n",
    "del(dfOutTempNorm)\n",
    "#\n",
    "dfBinNormTemp = dfOut[['timeAbsBin1count0to6', 'timeAbsBin2count6to12',     \\\n",
    "                       'timeAbsBin3count12to18', 'timeAbsBin4count18to24',  \\\n",
    "                        'timeAbsBin5count24plus']]\n",
    "#\n",
    "dfBinNormTemp['totalTimeAbsBin5count']   =      \\\n",
    "dfBinNormTemp['timeAbsBin1count0to6']    +      \\\n",
    "dfBinNormTemp['timeAbsBin2count6to12']   +      \\\n",
    "dfBinNormTemp['timeAbsBin3count12to18']  +      \\\n",
    "dfBinNormTemp['timeAbsBin4count18to24']  +      \\\n",
    "dfBinNormTemp['timeAbsBin5count24plus']\n",
    "#\n",
    "dfBinNormTemp['NORMtimeAbsBin1count0to6']   = dfBinNormTemp['timeAbsBin1count0to6']    / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin1count6to12']  = dfBinNormTemp['timeAbsBin2count6to12']   / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin1count12to18'] = dfBinNormTemp['timeAbsBin3count12to18']  / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin1count18to24'] = dfBinNormTemp['timeAbsBin4count18to24']  / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin5count24plus'] = dfBinNormTemp['timeAbsBin5count24plus']  / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "#\n",
    "dfOut['normTimeAbsBin1count0to6']   = dfBinNormTemp['NORMtimeAbsBin1count0to6']\n",
    "dfOut['normTimeAbsBin2count6to12']  = dfBinNormTemp['NORMtimeAbsBin1count6to12']\n",
    "dfOut['normTimeAbsBin3count12to18'] = dfBinNormTemp['NORMtimeAbsBin1count12to18']\n",
    "dfOut['normTimeAbsBin4count18to24'] = dfBinNormTemp['NORMtimeAbsBin1count18to24']\n",
    "dfOut['normTimeAbsBin5count24plus'] = dfBinNormTemp['NORMtimeAbsBin5count24plus']\n",
    "#\n",
    "del(dfBinNormTemp)\n",
    "#\n",
    "## normalise the span, avgTimeBetweenTweets, avgTimeOfNextTweet by first setting the -1 values to 0,\n",
    "#        applying min-max, then reinserting the -1 at the same positions as before\n",
    "dfOutTemp = dfOut[['span', 'avgTimeBetweenTweets', 'avgTimeOfNextTweet']]\n",
    "dfOutTemp['normSpan']                 = dfOutTemp['span']\n",
    "dfOutTemp['normAvgTimeBetweenTweets'] = dfOutTemp['avgTimeBetweenTweets']\n",
    "dfOutTemp['normAvgTimeOfNextTweet']   = dfOutTemp['avgTimeOfNextTweet']\n",
    "dfOutTemp['normSpan']                 = dfOutTemp['normSpan'].replace(-1, 0)\n",
    "dfOutTemp['normAvgTimeBetweenTweets'] = dfOutTemp['normAvgTimeBetweenTweets'].replace(-1, 0)\n",
    "dfOutTemp['normAvgTimeOfNextTweet']   = dfOutTemp['normAvgTimeOfNextTweet'].replace(-1, 0)\n",
    "dfOutTemp[['normSpan', 'normAvgTimeBetweenTweets', 'normAvgTimeOfNextTweet']] =            \\\n",
    "scaler.fit_transform(dfOutTemp[['normSpan', 'normAvgTimeBetweenTweets', 'normAvgTimeOfNextTweet']])\n",
    "dfOutTemp.loc[dfOutTemp['span'] == -1 , 'normSpan'] = -1\n",
    "dfOutTemp.loc[dfOutTemp['avgTimeBetweenTweets'] == -1 , 'normAvgTimeBetweenTweets'] = -1\n",
    "dfOutTemp.loc[dfOutTemp['avgTimeOfNextTweet'] == -1 , 'normAvgTimeOfNextTweet'] = -1\n",
    "dfOut['normSpan']                 = dfOutTemp['normSpan']\n",
    "dfOut['normAvgTimeBetweenTweets'] = dfOutTemp['normAvgTimeBetweenTweets']\n",
    "dfOut['normAvgTimeOfNextTweet']   = dfOutTemp['normAvgTimeOfNextTweet']\n",
    "#\n",
    "del(dfOutTemp)\n",
    "dfOut.rename(columns = {'search':'url'}, inplace = True)\n",
    "#\n",
    "## creating these additional fields which can be used for some data visualisation later\n",
    "dfOut['normPercentWeekDayByTotalTweets']   = dfOut['countWeekDay']   / dfOut['totalTweets']\n",
    "dfOut['normPercentWeekEndByTotalTweets']   = dfOut['countWeekEnd']   / dfOut['totalTweets']\n",
    "dfOut['normPercentMondayByTotalTweets']    = dfOut['countMonday']    / dfOut['totalTweets']\n",
    "dfOut['normPercentTuesdayByTotalTweets']   = dfOut['countTuesday']   / dfOut['totalTweets']\n",
    "dfOut['normPercentWednesdayByTotalTweets'] = dfOut['countWednesday'] / dfOut['totalTweets']\n",
    "dfOut['normPercentThursdayByTotalTweets']  = dfOut['countThursday']  / dfOut['totalTweets']\n",
    "dfOut['normPercentFridayByTotalTweets']    = dfOut['countFriday']    / dfOut['totalTweets']\n",
    "dfOut['normPercentSaturdayByTotalTweets']  = dfOut['countSaturday']  / dfOut['totalTweets']\n",
    "dfOut['normPercentSundayByTotalTweets']    = dfOut['countSunday']    / dfOut['totalTweets']\n",
    "#\n",
    "# deleting the urlType as it is garbage values\n",
    "del dfOut['urlType']\n",
    "del dfOut['domain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# writing the twitter features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opFileDfOutWITHNormValue = 'temp_T_features.csv'\n",
    "dfOut.to_csv(opFileDfOutWITHNormValue, index=False)\n",
    "del dfOut\n",
    "del dfIpData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M features -- extract MOPHOLOGICAL features (using spaCy) -- using the title+content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipDataFile = urlsFile4Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipInitialRows = 0\n",
    "data = pd.read_csv(ipDataFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "####################################################\n",
    "## define the functions to be used\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "#strip_html_tags('<html><h2>Some important text</h2></html>')\n",
    "#\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "#remove_accented_chars('Sómě Áccěntěd těxt')\n",
    "#\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        \n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        \n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    expanded_text = \"%s%s\" % (expanded_text[0].upper(), expanded_text[1:])\n",
    "\n",
    "    return expanded_text\n",
    "\n",
    "#expand_contractions(\"You all can't expand contractions I'd think\", contractions_dict)\n",
    "#\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "#remove_special_characters(\"Well this was fun! What do you think? 123#@!\", remove_digits=False)\n",
    "#\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "#lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")\n",
    "#\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "#remove_stopwords(\"The, and, if are stopwords, computer is not\")\n",
    "#\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=False, \n",
    "                     text_lemmatization=False, special_char_removal=False, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc, contractions_dict)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus\n",
    "#\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "nlp = en_core_web_lg.load()\n",
    "#\n",
    "data['full_text'] = data[\"title\"].map(str)+ '. ' + data[\"content\"]\n",
    "del data['title']\n",
    "del data['content']\n",
    "#\n",
    "data['clean_full_text'] = normalize_corpus(data['full_text'])\n",
    "del data['full_text']\n",
    "#\n",
    "dfOut = data.copy()\n",
    "## there are 50 features\n",
    "new_cols = ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``']\n",
    "dfOut = dfOut.assign(**dict.fromkeys(new_cols, -1.00))\n",
    "## no need to keep the data dataframe anymore\n",
    "del(data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printFreqValue = 1\n",
    "# POS tagging for sample cleaned full text\n",
    "l = 0\n",
    "#mf_clean_full_text = data3\n",
    "while l < len(dfOut):\n",
    "    sentence = str(dfOut.iloc[l].clean_full_text)\n",
    "    sentence_nlp = nlp(sentence)\n",
    "    # POS tagging with Spacy \n",
    "    spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "    postag_data = pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n",
    "    mb = postag_data.groupby(['POS tag']).count()\n",
    "    mc = mb.drop(['Tag type'], axis = 1)\n",
    "    mc = mc.rename_axis(\"\", axis=\"rows\").transpose()\n",
    "    mc.index = [l]\n",
    "    dfOut.update(mc)\n",
    "    l=l+1\n",
    "    if l % printFreqValue == 0:\n",
    "        print(f\"Finished row number = {l - 1}\\nfor url = {dfOut.iloc[l-1].url}\")\n",
    "#\n",
    "del dfOut['clean_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# renaming 6 columns to more meaningful names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The six columns, old and new names\n",
    "dfOut.rename(columns = { '$'  : 'dollar'},               inplace = True)\n",
    "dfOut.rename(columns = { \"''\" : 'dblApostrophe'},        inplace = True)\n",
    "dfOut.rename(columns = { ','  : 'comma'},                inplace = True)\n",
    "dfOut.rename(columns = { '.'  : 'dot'},                  inplace = True)\n",
    "dfOut.rename(columns = { ':'  : 'colon'},                inplace = True)\n",
    "dfOut.rename(columns = { '``' : 'dblSpecialApostrophe'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "dfOutTempNorm = dfOut.copy()\n",
    "del dfOutTempNorm['domain']\n",
    "del dfOutTempNorm['url']\n",
    "#\n",
    "dfOutTempNorm['normDollar']               = dfOutTempNorm['dollar']\n",
    "dfOutTempNorm['normDblApostrophe']        = dfOutTempNorm['dblApostrophe']\n",
    "dfOutTempNorm['normComma']                = dfOutTempNorm['comma']\n",
    "dfOutTempNorm['norm-LRB-']                = dfOutTempNorm['-LRB-']\n",
    "dfOutTempNorm['norm-RRB-']                = dfOutTempNorm['-RRB-']\n",
    "dfOutTempNorm['normDot']                  = dfOutTempNorm['dot']\n",
    "dfOutTempNorm['normColon']                = dfOutTempNorm['colon']\n",
    "dfOutTempNorm['normADD']                  = dfOutTempNorm['ADD']\n",
    "dfOutTempNorm['normAFX']                  = dfOutTempNorm['AFX']\n",
    "dfOutTempNorm['normCC']                   = dfOutTempNorm['CC']\n",
    "dfOutTempNorm['normCD']                   = dfOutTempNorm['CD']\n",
    "dfOutTempNorm['normDT']                   = dfOutTempNorm['DT']\n",
    "dfOutTempNorm['normEX']                   = dfOutTempNorm['EX']\n",
    "dfOutTempNorm['normFW']                   = dfOutTempNorm['FW']\n",
    "dfOutTempNorm['normHYPH']                 = dfOutTempNorm['HYPH']\n",
    "dfOutTempNorm['normIN']                   = dfOutTempNorm['IN']\n",
    "dfOutTempNorm['normJJ']                   = dfOutTempNorm['JJ']\n",
    "dfOutTempNorm['normJJR']                  = dfOutTempNorm['JJR']\n",
    "dfOutTempNorm['normJJS']                  = dfOutTempNorm['JJS']\n",
    "dfOutTempNorm['normLS']                   = dfOutTempNorm['LS']\n",
    "dfOutTempNorm['normMD']                   = dfOutTempNorm['MD']\n",
    "dfOutTempNorm['normNFP']                  = dfOutTempNorm['NFP']\n",
    "dfOutTempNorm['normNN']                   = dfOutTempNorm['NN']\n",
    "dfOutTempNorm['normNNP']                  = dfOutTempNorm['NNP']\n",
    "dfOutTempNorm['normNNPS']                 = dfOutTempNorm['NNPS']\n",
    "dfOutTempNorm['normNNS']                  = dfOutTempNorm['NNS']\n",
    "dfOutTempNorm['normPDT']                  = dfOutTempNorm['PDT']\n",
    "dfOutTempNorm['normPOS']                  = dfOutTempNorm['POS']\n",
    "dfOutTempNorm['normPRP']                  = dfOutTempNorm['PRP']\n",
    "dfOutTempNorm['normPRP$']                 = dfOutTempNorm['PRP$']\n",
    "dfOutTempNorm['normRB']                   = dfOutTempNorm['RB']\n",
    "dfOutTempNorm['normRBR']                  = dfOutTempNorm['RBR']\n",
    "dfOutTempNorm['normRBS']                  = dfOutTempNorm['RBS']\n",
    "dfOutTempNorm['normRP']                   = dfOutTempNorm['RP']\n",
    "dfOutTempNorm['normSYM']                  = dfOutTempNorm['SYM']\n",
    "dfOutTempNorm['normTO']                   = dfOutTempNorm['TO']\n",
    "dfOutTempNorm['normUH']                   = dfOutTempNorm['UH']\n",
    "dfOutTempNorm['normVB']                   = dfOutTempNorm['VB']\n",
    "dfOutTempNorm['normVBD']                  = dfOutTempNorm['VBD']\n",
    "dfOutTempNorm['normVBG']                  = dfOutTempNorm['VBG']\n",
    "dfOutTempNorm['normVBN']                  = dfOutTempNorm['VBN']\n",
    "dfOutTempNorm['normVBP']                  = dfOutTempNorm['VBP']\n",
    "dfOutTempNorm['normVBZ']                  = dfOutTempNorm['VBZ']\n",
    "dfOutTempNorm['normWDT']                  = dfOutTempNorm['WDT']\n",
    "dfOutTempNorm['normWP']                   = dfOutTempNorm['WP']\n",
    "dfOutTempNorm['normWP$']                  = dfOutTempNorm['WP$']\n",
    "dfOutTempNorm['normWRB']                  = dfOutTempNorm['WRB']\n",
    "dfOutTempNorm['normXX']                   = dfOutTempNorm['XX']\n",
    "dfOutTempNorm['norm_SP']                  = dfOutTempNorm['_SP']\n",
    "dfOutTempNorm['normDblSpecialApostrophe'] = dfOutTempNorm['dblSpecialApostrophe']\n",
    "#\n",
    "dfOutTempNorm['normDollar']               = dfOutTempNorm['normDollar'].replace(-1, 0)\n",
    "dfOutTempNorm['normDblApostrophe']        = dfOutTempNorm['normDblApostrophe'].replace(-1, 0)\n",
    "dfOutTempNorm['normComma']                = dfOutTempNorm['normComma'].replace(-1, 0)\n",
    "dfOutTempNorm['norm-LRB-']                = dfOutTempNorm['norm-LRB-'].replace(-1, 0)\n",
    "dfOutTempNorm['norm-RRB-']                = dfOutTempNorm['norm-RRB-'].replace(-1, 0)\n",
    "dfOutTempNorm['normDot']                  = dfOutTempNorm['normDot'].replace(-1, 0)\n",
    "dfOutTempNorm['normColon']                = dfOutTempNorm['normColon'].replace(-1, 0)\n",
    "dfOutTempNorm['normADD']                  = dfOutTempNorm['normADD'].replace(-1, 0)\n",
    "dfOutTempNorm['normAFX']                  = dfOutTempNorm['normAFX'].replace(-1, 0)\n",
    "dfOutTempNorm['normCC']                   = dfOutTempNorm['normCC'].replace(-1, 0)\n",
    "dfOutTempNorm['normCD']                   = dfOutTempNorm['normCD'].replace(-1, 0)\n",
    "dfOutTempNorm['normDT']                   = dfOutTempNorm['normDT'].replace(-1, 0)\n",
    "dfOutTempNorm['normEX']                   = dfOutTempNorm['normEX'].replace(-1, 0)\n",
    "dfOutTempNorm['normFW']                   = dfOutTempNorm['normFW'].replace(-1, 0)\n",
    "dfOutTempNorm['normHYPH']                 = dfOutTempNorm['normHYPH'].replace(-1, 0)\n",
    "dfOutTempNorm['normIN']                   = dfOutTempNorm['normIN'].replace(-1, 0)\n",
    "dfOutTempNorm['normJJ']                   = dfOutTempNorm['normJJ'].replace(-1, 0)\n",
    "dfOutTempNorm['normJJR']                  = dfOutTempNorm['normJJR'].replace(-1, 0)\n",
    "dfOutTempNorm['normJJS']                  = dfOutTempNorm['normJJS'].replace(-1, 0)\n",
    "dfOutTempNorm['normLS']                   = dfOutTempNorm['normLS'].replace(-1, 0)\n",
    "dfOutTempNorm['normMD']                   = dfOutTempNorm['normMD'].replace(-1, 0)\n",
    "dfOutTempNorm['normNFP']                  = dfOutTempNorm['normNFP'].replace(-1, 0)\n",
    "dfOutTempNorm['normNN']                   = dfOutTempNorm['normNN'].replace(-1, 0)\n",
    "dfOutTempNorm['normNNPS']                 = dfOutTempNorm['normNNPS'].replace(-1, 0)\n",
    "dfOutTempNorm['normNNS']                  = dfOutTempNorm['normNNS'].replace(-1, 0)\n",
    "dfOutTempNorm['normPDT']                  = dfOutTempNorm['normPDT'].replace(-1, 0)\n",
    "dfOutTempNorm['normPOS']                  = dfOutTempNorm['normPOS'].replace(-1, 0)\n",
    "dfOutTempNorm['normPRP']                  = dfOutTempNorm['normPRP'].replace(-1, 0)\n",
    "dfOutTempNorm['normPRP$']                 = dfOutTempNorm['normPRP$'].replace(-1, 0)\n",
    "dfOutTempNorm['normRB']                   = dfOutTempNorm['normRB'].replace(-1, 0)\n",
    "dfOutTempNorm['normRBR']                  = dfOutTempNorm['normRBR'].replace(-1, 0)\n",
    "dfOutTempNorm['normRBS']                  = dfOutTempNorm['normRBS'].replace(-1, 0)\n",
    "dfOutTempNorm['normRP']                   = dfOutTempNorm['normRP'].replace(-1, 0)\n",
    "dfOutTempNorm['normSYM']                  = dfOutTempNorm['normSYM'].replace(-1, 0)\n",
    "dfOutTempNorm['normTO']                   = dfOutTempNorm['normTO'].replace(-1, 0)\n",
    "dfOutTempNorm['normUH']                   = dfOutTempNorm['normUH'].replace(-1, 0)\n",
    "dfOutTempNorm['normVB']                   = dfOutTempNorm['normVB'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBD']                  = dfOutTempNorm['normVBD'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBG']                  = dfOutTempNorm['normVBG'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBN']                  = dfOutTempNorm['normVBN'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBP']                  = dfOutTempNorm['normVBP'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBZ']                  = dfOutTempNorm['normVBZ'].replace(-1, 0)\n",
    "dfOutTempNorm['normWDT']                  = dfOutTempNorm['normWDT'].replace(-1, 0)\n",
    "dfOutTempNorm['normWP']                   = dfOutTempNorm['normWP'].replace(-1, 0)\n",
    "dfOutTempNorm['normWP$']                  = dfOutTempNorm['normWP$'].replace(-1, 0)\n",
    "dfOutTempNorm['normWRB']                  = dfOutTempNorm['normWRB'].replace(-1, 0)\n",
    "dfOutTempNorm['normXX']                   = dfOutTempNorm['normXX'].replace(-1, 0)\n",
    "dfOutTempNorm['norm_SP']                  = dfOutTempNorm['norm_SP'].replace(-1, 0)\n",
    "dfOutTempNorm['normDblSpecialApostrophe'] = dfOutTempNorm['normDblSpecialApostrophe'].replace(-1, 0)\n",
    "#\n",
    "dfOutTempNorm[['normDollar', 'normDblApostrophe', 'normComma', 'norm-LRB-',\n",
    " 'norm-RRB-', 'normDot', 'normColon', 'normADD', 'normAFX', 'normCC', 'normCD',\n",
    " 'normDT', 'normEX', 'normFW', 'normHYPH', 'normIN', 'normJJ', 'normJJR', 'normJJS',\n",
    " 'normLS', 'normMD', 'normNFP', 'normNN', 'normNNP', 'normNNPS', 'normNNS', 'normPDT',\n",
    " 'normPOS', 'normPRP', 'normPRP$', 'normRB', 'normRBR', 'normRBS', 'normRP',\n",
    " 'normSYM', 'normTO', 'normUH', 'normVB', 'normVBD', 'normVBG', 'normVBN',\n",
    " 'normVBP', 'normVBZ', 'normWDT', 'normWP', 'normWP$', 'normWRB',\n",
    " 'normXX', 'norm_SP', 'normDblSpecialApostrophe']] = \\\n",
    "scaler.fit_transform(dfOutTempNorm[['normDollar', 'normDblApostrophe', 'normComma', 'norm-LRB-',\n",
    " 'norm-RRB-', 'normDot', 'normColon', 'normADD', 'normAFX', 'normCC', 'normCD',\n",
    " 'normDT', 'normEX', 'normFW', 'normHYPH', 'normIN', 'normJJ', 'normJJR', 'normJJS',\n",
    " 'normLS', 'normMD', 'normNFP', 'normNN', 'normNNP', 'normNNPS', 'normNNS', 'normPDT',\n",
    " 'normPOS', 'normPRP', 'normPRP$', 'normRB', 'normRBR', 'normRBS', 'normRP',\n",
    " 'normSYM', 'normTO', 'normUH', 'normVB', 'normVBD', 'normVBG', 'normVBN',\n",
    " 'normVBP', 'normVBZ', 'normWDT', 'normWP', 'normWP$', 'normWRB',\n",
    " 'normXX', 'norm_SP', 'normDblSpecialApostrophe']])\n",
    "#\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dollar']               == -1 , 'normDollar'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dblApostrophe']        == -1 , 'normDblApostrophe'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['comma']                == -1 , 'normComma'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['-LRB-']                == -1 , 'norm-LRB-'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['-RRB-']                == -1 , 'norm-RRB-'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dot']                  == -1 , 'normDot'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['colon']                == -1 , 'normColon'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['ADD']                  == -1 , 'normADD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['AFX']                  == -1 , 'normAFX'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['CC']                   == -1 , 'normCC'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['CD']                   == -1 , 'normCD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['DT']                   == -1 , 'normDT'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['EX']                   == -1 , 'normEX'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['FW']                   == -1 , 'normFW'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['HYPH']                 == -1 , 'normHYPH'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['IN']                   == -1 , 'normIN'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['JJ']                   == -1 , 'normJJ'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['JJR']                  == -1 , 'normJJR'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['JJS']                  == -1 , 'normJJS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['LS']                   == -1 , 'normLS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['MD']                   == -1 , 'normMD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NFP']                  == -1 , 'normNFP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NN']                   == -1 , 'normNN'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NNPS']                 == -1 , 'normNNPS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NNS']                  == -1 , 'normNNS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['PDT']                  == -1 , 'normPDT'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['POS']                  == -1 , 'normPOS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['PRP']                  == -1 , 'normPRP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['PRP$']                 == -1 , 'normPRP$'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RB']                   == -1 , 'normRB'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RBR']                  == -1 , 'normRBR'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RBS']                  == -1 , 'normRBS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RP']                   == -1 , 'normRP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['SYM']                  == -1 , 'normSYM'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['TO']                   == -1 , 'normTO'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['UH']                   == -1 , 'normUH'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VB']                   == -1 , 'normVB'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBD']                  == -1 , 'normVBD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBG']                  == -1 , 'normVBG'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBN']                  == -1 , 'normVBN'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBP']                  == -1 , 'normVBP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBZ']                  == -1 , 'normVBZ'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WDT']                  == -1 , 'normWDT'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WP']                   == -1 , 'normWP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WP$']                  == -1 , 'normWP$'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WRB']                  == -1 , 'normWRB'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['XX']                   == -1 , 'normXX'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['_SP']                  == -1 , 'norm_SP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dblSpecialApostrophe'] == -1 , 'normDblSpecialApostrophe'] = -1\n",
    "#\n",
    "dfOut['normDollar']               = dfOutTempNorm['normDollar']\n",
    "dfOut['normDblApostrophe']        = dfOutTempNorm['normDblApostrophe']\n",
    "dfOut['normComma']                = dfOutTempNorm['normComma']\n",
    "dfOut['norm-LRB-']                = dfOutTempNorm['norm-LRB-']\n",
    "dfOut['norm-RRB-']                = dfOutTempNorm['norm-RRB-']\n",
    "dfOut['normDot']                  = dfOutTempNorm['normDot']\n",
    "dfOut['normColon']                = dfOutTempNorm['normColon']\n",
    "dfOut['normADD']                  = dfOutTempNorm['normADD']\n",
    "dfOut['normAFX']                  = dfOutTempNorm['normAFX']\n",
    "dfOut['normCC']                   = dfOutTempNorm['normCC']\n",
    "dfOut['normCD']                   = dfOutTempNorm['normCD']\n",
    "dfOut['normDT']                   = dfOutTempNorm['normDT']\n",
    "dfOut['normEX']                   = dfOutTempNorm['normEX']\n",
    "dfOut['normFW']                   = dfOutTempNorm['normFW']\n",
    "dfOut['normHYPH']                 = dfOutTempNorm['normHYPH']\n",
    "dfOut['normIN']                   = dfOutTempNorm['normIN']\n",
    "dfOut['normJJ']                   = dfOutTempNorm['normJJ']\n",
    "dfOut['normJJR']                  = dfOutTempNorm['normJJR']\n",
    "dfOut['normJJS']                  = dfOutTempNorm['normJJS']\n",
    "dfOut['normLS']                   = dfOutTempNorm['normLS']\n",
    "dfOut['normMD']                   = dfOutTempNorm['normMD']\n",
    "dfOut['normNFP']                  = dfOutTempNorm['normNFP']\n",
    "dfOut['normNN']                   = dfOutTempNorm['normNN']\n",
    "dfOut['normNNP']                  = dfOutTempNorm['normNNP']\n",
    "dfOut['normNNPS']                 = dfOutTempNorm['normNNPS']\n",
    "dfOut['normNNS']                  = dfOutTempNorm['normNNS']\n",
    "dfOut['normPDT']                  = dfOutTempNorm['normPDT']\n",
    "dfOut['normPOS']                  = dfOutTempNorm['normPOS']\n",
    "dfOut['normPRP']                  = dfOutTempNorm['normPRP']\n",
    "dfOut['normPRP$']                 = dfOutTempNorm['normPRP$']\n",
    "dfOut['normRB']                   = dfOutTempNorm['normRB']\n",
    "dfOut['normRBR']                  = dfOutTempNorm['normRBR']\n",
    "dfOut['normRBS']                  = dfOutTempNorm['normRBS']\n",
    "dfOut['normRP']                   = dfOutTempNorm['normRP']\n",
    "dfOut['normSYM']                  = dfOutTempNorm['normSYM']\n",
    "dfOut['normTO']                   = dfOutTempNorm['normTO']\n",
    "dfOut['normUH']                   = dfOutTempNorm['normUH']\n",
    "dfOut['normVB']                   = dfOutTempNorm['normVB']\n",
    "dfOut['normVBD']                  = dfOutTempNorm['normVBD']\n",
    "dfOut['normVBG']                  = dfOutTempNorm['normVBG']\n",
    "dfOut['normVBN']                  = dfOutTempNorm['normVBN']\n",
    "dfOut['normVBP']                  = dfOutTempNorm['normVBP']\n",
    "dfOut['normVBZ']                  = dfOutTempNorm['normVBZ']\n",
    "dfOut['normWDT']                  = dfOutTempNorm['normWDT']\n",
    "dfOut['normWP']                   = dfOutTempNorm['normWP']\n",
    "dfOut['normWP$']                  = dfOutTempNorm['normWP$']\n",
    "dfOut['normWRB']                  = dfOutTempNorm['normWRB']\n",
    "dfOut['normXX']                   = dfOutTempNorm['normXX']\n",
    "dfOut['norm_SP']                  = dfOutTempNorm['norm_SP']\n",
    "dfOut['normDblSpecialApostrophe'] = dfOutTempNorm['normDblSpecialApostrophe']\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writing the morphological features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile_WITH_norm = 'temp_M_features.csv'\n",
    "dfOut.to_csv(outFile_WITH_norm, index=False)\n",
    "del dfOut\n",
    "del dfOutTempNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R features -- extract READABILITY features (using Textstat) -- using the title+content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Use this script to process the datasets for READABILITY FEATURES EXTRACTION.\n",
    "#Run the rogram with EXACTLY 5 argruments as shown below:\n",
    "#python <script> <input-file-name> <output-CSV-file> <#skipInitialRows> <#rowsToProcess> <#rowsAfterWhichToShowPrintMessageTracker>\n",
    "\n",
    "#skipInitialRows : is an integer. Is the CSV data row number (0 being for header) from which the data should be loaded. Here 0 is the header so to skip first 5 data rows: skipInitialRows = 5.\n",
    "#rowsToProcess : is an integer. If = -1 then will read in all the rows after skipping as per above parameter. Otherwise, will read in the number specified.\n",
    "\n",
    "#The packages expected by the script are:\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import spacy\n",
    "#import nltk\n",
    "#from nltk.tokenize.toktok import ToktokTokenizer\n",
    "#import re\n",
    "#from bs4 import BeautifulSoup\n",
    "#import unicodedata\n",
    "#import en_core_web_lg\n",
    "#import en_core_web_sm\n",
    "#from contractions import contractions_dict\n",
    "#from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#import csv\n",
    "#import sys\n",
    "#from datetime import datetime\n",
    "#import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python script will run and create the readability features file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the print statements about number columns in the dataframe.shape shows a value greater\n",
    "###     by one (4 instead of 3, 23 instead of 22). This is fine. The script was written expecting the\n",
    "###     input data to file to hold only the columns: domain, url, title and content.\n",
    "###     But here was are running with an additional column for urlType and that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scriptCreateReadabilityFeatures.py demo_input_data_v1.csv temp_R_features.csv 0 15 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L features -- extract PSYCHOLOGICAL features (using LIWC)   -- using the title+content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This is a paid tool and there is no script for it. You pass it a CSV file with the textual data to be analyzed and it returns 50 features.\n",
    "#We are concatenating the Title+Content into so called \"Full_Text\" and using the tool to extract the features for each URL.\n",
    "#Thus, after the Psychological features are extracted using LIWC, it should be saved as a file: temp_L_features.csv.\n",
    "\n",
    "#The columns of the file should be as follows:\n",
    "\n",
    "#So we will use the input file which has the domain, url, title, content.\n",
    "#Load it into dataframe, combine the title+content as \"full_text\".\n",
    "#Perform some basic cleaning on the \"full_text\" to create a new column \"clean_full_text\".\n",
    "#Then this columns data is extracted as a csv file containing only this \"clean_full_text\".\n",
    "#LIWC tool returns a csv file with columns as \"clean_full_text\" unchanged, followed by\n",
    "#93 features per row.\n",
    "\n",
    "#This output file is the \"temp_L_features.csv\" which is used in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the invidual features files for L, M, R and T into one file.\n",
    "\n",
    "## Merging all the individual feature files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderPath = ''\n",
    "skipInitialRows = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the features in order of M , L , T , R\n",
    "inFile = folderPath + 'temp_M_features.csv'\n",
    "dfTempM = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)\n",
    "inFile = folderPath + 'temp_L_features.csv'\n",
    "dfTempL = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)\n",
    "inFile = folderPath + 'temp_T_features.csv'\n",
    "dfTempT = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)\n",
    "inFile = folderPath + 'temp_R_features.csv'\n",
    "dfTempR = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# see the columns for each of the files\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempM) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"M features = \\n{outColStr}\\n\")\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempL) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"L features = \\n{outColStr}\\n\")\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempT) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"T features = \\n{outColStr}\\n\")\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempR) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"R features = \\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this stage domain and urlType exist in multiple files, but we want to retain only the M file ones.\n",
    "# delete the domain repetition and only retain it in the T file\n",
    "del dfTempL['domain']\n",
    "del dfTempR['domain']\n",
    "del dfTempR['urlType']\n",
    "# T file does not have domain.\n",
    "#\n",
    "dfTempOut = pd.merge(dfTempM, dfTempL, on=['url'])\n",
    "dfTempOut1 = pd.merge(dfTempOut, dfTempT, on=['url'])\n",
    "del dfTempOut\n",
    "df_ALL_Out_WITH_norm = pd.merge(dfTempOut1, dfTempR, on=['url'])\n",
    "del dfTempOut1\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# see the columns for each of the files\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_WITH_norm features = \\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsList_ALL_WITH_norm = []\n",
    "# expecting the order of features as Morphological , LIWC (Psychological) , Twitter , Readability\n",
    "preTagList = ['' , 'M_' , 'L_' , 'T_' , 'R_']\n",
    "idxPreTagList = 0\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col == 'dollar':                            ## first column of M Morphological\n",
    "        idxPreTagList = 1\n",
    "    elif col == 'WC':                              ## first column of L LIWC\n",
    "        idxPreTagList = 2\n",
    "    elif col == 'totalTweets':                     ## first column of T Twitter\n",
    "        idxPreTagList = 3\n",
    "    elif col == 'full_text_Flesch_reading_ease':   ## first column of R Readability\n",
    "        idxPreTagList = 4\n",
    "    newCol = preTagList[idxPreTagList] + col\n",
    "    colsList_ALL_WITH_norm.append(newCol)\n",
    "#\n",
    "# Update the df_All_Out columns\n",
    "# Thus the All features WITH norm dataframe is ready\n",
    "#\n",
    "df_ALL_Out_WITH_norm.columns = colsList_ALL_WITH_norm\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# see the updated column names for all the features\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_WITH_norm features after updation =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify ALL features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_ALL_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_ALL_NO_norm.append(col)\n",
    "    elif \"norm\" not in col :\n",
    "        colsList_ALL_NO_norm.append(col)\n",
    "#\n",
    "df_ALL_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_ALL_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(colsList_ALL_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create all the data files to be used for training or testing new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderPath = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to CSV: All features WITH norm    and   other file for All features WITHOUT norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALL_Out_WITH_norm.to_csv(folderPath + 'All_Features_WITH_norm.csv' , index = True)\n",
    "df_ALL_Out_NO_norm.to_csv(folderPath + 'All_Features_NO_norm.csv' , index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ALL_Out_WITH_norm , df_ALL_Out_NO_norm, dfTempM , dfTempL , dfTempT , dfTempR\n",
    "del colsList_ALL_WITH_norm, colsList_ALL_NO_norm, outColStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the All_Features file into a dataframe and process from there to create\n",
    "#      the individual group feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALL_Out_WITH_norm = pd.read_csv(folderPath + 'All_Features_WITH_norm.csv' , index_col = 0, sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALL_Out_WITH_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outColStr = \"\"\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_WITH_norm features after updation =\\n{outColStr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify L features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify all L features and write to CSV file\n",
    "colsList_L_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_L_WITH_norm.append(col)\n",
    "    elif col.startswith('L_'):\n",
    "        colsList_L_WITH_norm.append(col)\n",
    "#\n",
    "df_L_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_L_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_L_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_L_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify L features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_L_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_L_NO_norm.append(col)\n",
    "    elif col.startswith('L_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_L_NO_norm.append(col)\n",
    "#\n",
    "df_L_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_L_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_L_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_L_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to CSV: L features WITH norm    and   other file for L features WITHOUT norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_L_Out_WITH_norm.to_csv(folderPath + 'L_Features_WITH_norm.csv' , index = True)\n",
    "del df_L_Out_WITH_norm\n",
    "df_L_Out_NO_norm.to_csv(folderPath + 'L_Features_NO_norm.csv' , index = True)\n",
    "del df_L_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify M features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all M features and write to CSV file\n",
    "colsList_M_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_M_WITH_norm.append(col)\n",
    "    elif col.startswith('M_'):\n",
    "        colsList_M_WITH_norm.append(col)\n",
    "#\n",
    "df_M_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_M_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_M_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_M_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify M features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_M_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_M_NO_norm.append(col)\n",
    "    elif col.startswith('M_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_M_NO_norm.append(col)\n",
    "#\n",
    "df_M_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_M_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_M_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_M_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to CSV: M features WITH norm    and   other file for M features WITHOUT norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M_Out_WITH_norm.to_csv(folderPath + 'M_Features_WITH_norm.csv' , index = True)\n",
    "del df_M_Out_WITH_norm\n",
    "df_M_Out_NO_norm.to_csv(folderPath + 'M_Features_NO_norm.csv' , index = True)\n",
    "del df_M_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify R features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all R features and write to CSV file\n",
    "colsList_R_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_R_WITH_norm.append(col)\n",
    "    elif col.startswith('R_'):\n",
    "        colsList_R_WITH_norm.append(col)\n",
    "#\n",
    "df_R_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_R_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_R_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_R_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify R features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_R_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_R_NO_norm.append(col)\n",
    "    elif col.startswith('R_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_R_NO_norm.append(col)\n",
    "#\n",
    "df_R_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_R_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_R_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_R_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R_Out_WITH_norm.to_csv(folderPath + 'R_Features_WITH_norm.csv' , index = True)\n",
    "del df_R_Out_WITH_norm\n",
    "df_R_Out_NO_norm.to_csv(folderPath + 'R_Features_NO_norm.csv' , index = True)\n",
    "del df_R_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify T features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all T features and write to CSV file\n",
    "colsList_T_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_T_WITH_norm.append(col)\n",
    "    elif col.startswith('T_'):\n",
    "        colsList_T_WITH_norm.append(col)\n",
    "#\n",
    "df_T_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_T_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_T_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_T_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify T features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_T_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_T_NO_norm.append(col)\n",
    "    elif col.startswith('T_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_T_NO_norm.append(col)\n",
    "#\n",
    "df_T_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_T_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_T_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_T_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T_Out_WITH_norm.to_csv(folderPath + 'T_Features_WITH_norm.csv' , index = True)\n",
    "del df_T_Out_WITH_norm\n",
    "df_T_Out_NO_norm.to_csv(folderPath + 'T_Features_NO_norm.csv' , index = True)\n",
    "del df_T_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify MLR features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all MLR features and write to CSV file\n",
    "colsList_MLR_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_MLR_WITH_norm.append(col)\n",
    "    elif col.startswith('M_') or col.startswith('L_') or col.startswith('R_'):\n",
    "        colsList_MLR_WITH_norm.append(col)\n",
    "#\n",
    "df_MLR_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_MLR_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_MLR_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_MLR_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify MLR features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_MLR_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_MLR_NO_norm.append(col)\n",
    "    elif col.startswith('M_') or col.startswith('L_') or col.startswith('R_') :\n",
    "        if \"norm\" not in col :\n",
    "            colsList_MLR_NO_norm.append(col)\n",
    "#\n",
    "df_MLR_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_MLR_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_MLR_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_MLR_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLR_Out_WITH_norm.to_csv(folderPath + 'MLR_Features_WITH_norm.csv' , index = True)\n",
    "del df_MLR_Out_WITH_norm\n",
    "df_MLR_Out_NO_norm.to_csv(folderPath + 'MLR_Features_NO_norm.csv' , index = True)\n",
    "del df_MLR_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now all the output files with features are ready:\n",
    "### For WITH normal : one each for All, M, L, T, R, MLR = 6 files\n",
    "### For NO   normal : one each for All, M, L, T, R, MLR = 6 files\n",
    "### Thus total 12 files\n",
    "## ------------------------------------------------------------------------\n",
    "# We will be using the NO normal versions for testing the accuracy of models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
